{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc376e3",
   "metadata": {},
   "source": [
    "# Data Pre-processing Notebook\n",
    "In this notebook I'm running the data pre-processing phase on the Randomized DCE dataset.\n",
    "\n",
    "**Author**: Arthur G."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2190c",
   "metadata": {},
   "source": [
    "## Loading Dependencies\n",
    "Importing and setting up all the dependencies for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627b80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24003c63",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "A set of helper functions to automate data loading and pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904cc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_class(\n",
    "    name: str, \n",
    "    mapped_data: dict, \n",
    "    hdf_data_object: h5py._hl.group.Group\n",
    ") -> list:\n",
    "    \"\"\"Get a subject ID and loads it's associated data and class.\"\"\"\n",
    "    SUBJECT_IDS = list(mapped_data.keys())\n",
    "    \n",
    "    # data storages\n",
    "    subjects_data_arrays = []\n",
    "    subject_data_targets = []\n",
    "    \n",
    "    # reading data arrays and targets\n",
    "    for subject_id in SUBJECT_IDS:\n",
    "        subjects_data_arrays.append(np.array(hdf_data_object.get(subject_id)))\n",
    "        subject_data_targets.append(mapped_data[subject_id])\n",
    "    \n",
    "    # checking the subject with the smallest record for normalized windowing\n",
    "    min_subject_cols = min([size.shape[-1] for size in subjects_data_arrays])\n",
    "    current_subject_rows = subjects_data_arrays[0].shape[0]\n",
    "    print(f\"For '{name}', the smallest record is: {min_subject_cols} (with {current_subject_rows} rows)\")\n",
    "        \n",
    "    return subjects_data_arrays, subject_data_targets\n",
    "\n",
    "\n",
    "def calc_window_number(name: str, base_record_size: int) -> None:\n",
    "    \"\"\"Calculates all the exact number of windows a given record can be splitted into.\"\"\"\n",
    "    possible_samples = []\n",
    "    \n",
    "    for num in range(1, base_record_size+1):\n",
    "        res = base_record_size % num\n",
    "        \n",
    "        if res == 0:\n",
    "            window_size = base_record_size // num\n",
    "            possible_samples.append(f\"samples: {num} (size: {window_size})\")          \n",
    "            \n",
    "    print(f\"{name} possible windowing strategy: \\n {possible_samples} \\n\\n\")\n",
    "    \n",
    "\n",
    "def array_to_img(\n",
    "    data_matrix: np.ndarray,\n",
    "    target_value: str,\n",
    "    n_samples: int,\n",
    "    rows: int,\n",
    "    cols: int, \n",
    "    axis: int = 1\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert numpy data matrix to images dataset.\"\"\"\n",
    "    if axis not in [0, 1]:\n",
    "        raise Exception(\"Axis not valid!\")\n",
    "    \n",
    "    # processing data matrix\n",
    "    raw_data_matrix = data_matrix[:,0:cols]\n",
    "    samples_col = cols // n_samples\n",
    "    splitted_data_matrix = np.array_split(raw_data_matrix, n_samples, axis=axis)\n",
    "    data = np.array(splitted_data_matrix).reshape(((n_samples, 1, samples_col, rows)))\n",
    "    \n",
    "    # processing sample's target\n",
    "    target = np.array([target_value]*n_samples)\n",
    "        \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd96ef3",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Loading the raw randomized dataset from the H5 file as well as it's separated metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df81e7",
   "metadata": {},
   "source": [
    "Loading metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc9603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Operador</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Produtividade</th>\n",
       "      <th>prodClassesArray de</th>\n",
       "      <th>prodClassesArray até</th>\n",
       "      <th>DCEDataArray de</th>\n",
       "      <th>DCEDataArray até</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sujeito01_AAB_36_GCA_RT_OA_ICA_C3</td>\n",
       "      <td>B</td>\n",
       "      <td>9751.651897</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>53100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sujeito01_AAB_36_GCA_RTpos_OA_ICA_C3</td>\n",
       "      <td>B</td>\n",
       "      <td>9751.651897</td>\n",
       "      <td>178</td>\n",
       "      <td>355</td>\n",
       "      <td>53101</td>\n",
       "      <td>106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sujeito01_AAB_36_GCD_RT_OA_ICA_C3</td>\n",
       "      <td>B</td>\n",
       "      <td>9647.243800</td>\n",
       "      <td>356</td>\n",
       "      <td>532</td>\n",
       "      <td>106501</td>\n",
       "      <td>159600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sujeito01_AAB_36_GCD_RTpos_OA_ICA_C3</td>\n",
       "      <td>B</td>\n",
       "      <td>9647.243800</td>\n",
       "      <td>533</td>\n",
       "      <td>709</td>\n",
       "      <td>159601</td>\n",
       "      <td>212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sujeito02_ASC_33_GCA_RT_OA_ICA_C3</td>\n",
       "      <td>D</td>\n",
       "      <td>6523.439700</td>\n",
       "      <td>710</td>\n",
       "      <td>887</td>\n",
       "      <td>212701</td>\n",
       "      <td>266100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Operador Classe  Produtividade  \\\n",
       "0     Sujeito01_AAB_36_GCA_RT_OA_ICA_C3      B    9751.651897   \n",
       "1  Sujeito01_AAB_36_GCA_RTpos_OA_ICA_C3      B    9751.651897   \n",
       "2     Sujeito01_AAB_36_GCD_RT_OA_ICA_C3      B    9647.243800   \n",
       "3  Sujeito01_AAB_36_GCD_RTpos_OA_ICA_C3      B    9647.243800   \n",
       "4     Sujeito02_ASC_33_GCA_RT_OA_ICA_C3      D    6523.439700   \n",
       "\n",
       "   prodClassesArray de  prodClassesArray até  DCEDataArray de  \\\n",
       "0                    1                   177                1   \n",
       "1                  178                   355            53101   \n",
       "2                  356                   532           106501   \n",
       "3                  533                   709           159601   \n",
       "4                  710                   887           212701   \n",
       "\n",
       "   DCEDataArray até  \n",
       "0             53100  \n",
       "1            106500  \n",
       "2            159600  \n",
       "3            212700  \n",
       "4            266100  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subjects metadata.\n",
    "metadata = pd.read_excel(os.path.join(\"..\", \"references\", \"cognitive_training_metadata.xlsx\"))\n",
    "metadata[\"Operador\"] = metadata.Operador.map(lambda x: x.replace(\"'\", \"\"))\n",
    "metadata[\"Classe\"] = metadata.Classe.map(lambda x: x.replace(\"'\", \"\"))\n",
    "\n",
    "# mapping subject id to class\n",
    "subject_to_class_map = metadata[[\"Operador\", \"Classe\"]].set_index(\"Operador\").to_dict()[\"Classe\"]\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d1e0b",
   "metadata": {},
   "source": [
    "Loading randomized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b897211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Original data', the smallest record is: 53228 (with 6 rows)\n",
      "For 'Similar M data', the smallest record is: 52862 (with 12 rows)\n",
      "For 'Large Tau data', the smallest record is: 52862 (with 6 rows)\n",
      "For 'Smaller Tau data', the smallest record is: 53512 (with 6 rows)\n",
      "For 'Similar Tau data', the smallest record is: 53216 (with 6 rows)\n"
     ]
    }
   ],
   "source": [
    "# reading h5py file\n",
    "data = h5py.File(os.path.join(\"..\", \"data\", \"raw\", \"bancoDCEComParametrosRandomizados.h5\"), \"r\")\n",
    "\n",
    "# reading each randomized dataset\n",
    "original_data, original_target = get_subject_class(\"Original data\", subject_to_class_map, data.get(\"Original\"))\n",
    "similar_m_data, similar_m_target = get_subject_class(\"Similar M data\", subject_to_class_map, data.get(\"M rand Similar\"))\n",
    "large_tau_data, large_tau_target = get_subject_class(\"Large Tau data\", subject_to_class_map, data.get(\"Tau rand Grande\"))\n",
    "smaller_tau_data, smaller_tau_target = get_subject_class(\"Smaller Tau data\", subject_to_class_map, data.get(\"Tau rand Menor\"))\n",
    "similar_tau_data, similar_tau_target = get_subject_class(\"Similar Tau data\", subject_to_class_map, data.get(\"Tau rand Similar\"))\n",
    "\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f4df0",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "In this section I'm running the data pre-processing step, which is comprised of:\n",
    "+ Windowing of each subset (normalized by the subject with the smallest record from each group).\n",
    "+ Organization into different dataset files for further machine learning modeling.\n",
    "\n",
    "I'm starting by figuring out the *\"kind of\"* optimal number of windows for each subset of raw data. *Based on some tests I did beforehand, I needed to change the base record size number (the number of columns of the subject with the smallest record)* used to normalize the split because some of the subsets ended up with a too small number of columns, which may negatively impact the performance of our algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e1d69b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data possible windowing strategy: \n",
      " ['samples: 1 (size: 52839)', 'samples: 3 (size: 17613)', 'samples: 9 (size: 5871)', 'samples: 19 (size: 2781)', 'samples: 27 (size: 1957)', 'samples: 57 (size: 927)', 'samples: 103 (size: 513)', 'samples: 171 (size: 309)', 'samples: 309 (size: 171)', 'samples: 513 (size: 103)', 'samples: 927 (size: 57)', 'samples: 1957 (size: 27)', 'samples: 2781 (size: 19)', 'samples: 5871 (size: 9)', 'samples: 17613 (size: 3)', 'samples: 52839 (size: 1)'] \n",
      "\n",
      "\n",
      "Similar M data possible windowing strategy: \n",
      " ['samples: 1 (size: 52839)', 'samples: 3 (size: 17613)', 'samples: 9 (size: 5871)', 'samples: 19 (size: 2781)', 'samples: 27 (size: 1957)', 'samples: 57 (size: 927)', 'samples: 103 (size: 513)', 'samples: 171 (size: 309)', 'samples: 309 (size: 171)', 'samples: 513 (size: 103)', 'samples: 927 (size: 57)', 'samples: 1957 (size: 27)', 'samples: 2781 (size: 19)', 'samples: 5871 (size: 9)', 'samples: 17613 (size: 3)', 'samples: 52839 (size: 1)'] \n",
      "\n",
      "\n",
      "Large Tau data possible windowing strategy: \n",
      " ['samples: 1 (size: 52839)', 'samples: 3 (size: 17613)', 'samples: 9 (size: 5871)', 'samples: 19 (size: 2781)', 'samples: 27 (size: 1957)', 'samples: 57 (size: 927)', 'samples: 103 (size: 513)', 'samples: 171 (size: 309)', 'samples: 309 (size: 171)', 'samples: 513 (size: 103)', 'samples: 927 (size: 57)', 'samples: 1957 (size: 27)', 'samples: 2781 (size: 19)', 'samples: 5871 (size: 9)', 'samples: 17613 (size: 3)', 'samples: 52839 (size: 1)'] \n",
      "\n",
      "\n",
      "Smaller Tau data possible windowing strategy: \n",
      " ['samples: 1 (size: 52839)', 'samples: 3 (size: 17613)', 'samples: 9 (size: 5871)', 'samples: 19 (size: 2781)', 'samples: 27 (size: 1957)', 'samples: 57 (size: 927)', 'samples: 103 (size: 513)', 'samples: 171 (size: 309)', 'samples: 309 (size: 171)', 'samples: 513 (size: 103)', 'samples: 927 (size: 57)', 'samples: 1957 (size: 27)', 'samples: 2781 (size: 19)', 'samples: 5871 (size: 9)', 'samples: 17613 (size: 3)', 'samples: 52839 (size: 1)'] \n",
      "\n",
      "\n",
      "Similar Tau data possible windowing strategy: \n",
      " ['samples: 1 (size: 52839)', 'samples: 3 (size: 17613)', 'samples: 9 (size: 5871)', 'samples: 19 (size: 2781)', 'samples: 27 (size: 1957)', 'samples: 57 (size: 927)', 'samples: 103 (size: 513)', 'samples: 171 (size: 309)', 'samples: 309 (size: 171)', 'samples: 513 (size: 103)', 'samples: 927 (size: 57)', 'samples: 1957 (size: 27)', 'samples: 2781 (size: 19)', 'samples: 5871 (size: 9)', 'samples: 17613 (size: 3)', 'samples: 52839 (size: 1)'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calc_window_number(\"Original data\", 52839)\n",
    "calc_window_number(\"Similar M data\", 52839)\n",
    "calc_window_number(\"Large Tau data\", 52839)\n",
    "calc_window_number(\"Smaller Tau data\", 52839)\n",
    "calc_window_number(\"Similar Tau data\", 52839)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767718",
   "metadata": {},
   "source": [
    "### Samples Split\n",
    "In this step I'm splitting the raw DCE data into samples for AI/ML modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08af9e7",
   "metadata": {},
   "source": [
    "Starting with the original DCE data (calculated based on the Take's Theorem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e32ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (30780, 1, 103, 6)\n"
     ]
    }
   ],
   "source": [
    "original_data_samples = []\n",
    "original_data_target = []\n",
    "\n",
    "for idx in np.arange(len(original_data)):\n",
    "    # splitting samples for original data\n",
    "    current_samples, current_targets = array_to_img(\n",
    "        original_data[idx],\n",
    "        original_target[idx],\n",
    "        n_samples=513, \n",
    "        rows=6, \n",
    "        cols=52839\n",
    "    )\n",
    "    \n",
    "    # storing original data\n",
    "    original_data_samples.append(current_samples)\n",
    "    [original_data_target.append(target) for target in current_targets]\n",
    "    \n",
    "original_data_samples = np.vstack(original_data_samples)\n",
    "original_data_target = np.array(original_data_target)\n",
    "\n",
    "print(f\"Original data shape: {original_data_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6922f",
   "metadata": {},
   "source": [
    "Now for the Similar M value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1478adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar M data shape: (30780, 1, 103, 12)\n"
     ]
    }
   ],
   "source": [
    "similar_m_data_samples = []\n",
    "similar_m_data_target = []\n",
    "\n",
    "for idx in np.arange(len(original_data)):\n",
    "    # splitting samples for similar m data\n",
    "    current_samples, current_targets = array_to_img(\n",
    "        similar_m_data[idx],\n",
    "        similar_m_target[idx],\n",
    "        n_samples=513, \n",
    "        rows=12, \n",
    "        cols=52839\n",
    "    )\n",
    "    \n",
    "    # storing similar m data\n",
    "    similar_m_data_samples.append(current_samples)\n",
    "    [similar_m_data_target.append(target) for target in current_targets]\n",
    "\n",
    "similar_m_data_samples = np.vstack(similar_m_data_samples)\n",
    "similar_m_data_target = np.array(similar_m_data_target)\n",
    "\n",
    "print(f\"Similar M data shape: {similar_m_data_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b01217",
   "metadata": {},
   "source": [
    "And now for large Tau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d017ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Tau data shape: (30780, 1, 103, 6)\n"
     ]
    }
   ],
   "source": [
    "large_tau_data_samples = []\n",
    "large_tau_data_target = []\n",
    "\n",
    "for idx in np.arange(len(original_data)):\n",
    "    # splitting samples for large tau data\n",
    "    current_samples, current_targets = array_to_img(\n",
    "        large_tau_data[idx],\n",
    "        large_tau_target[idx],\n",
    "        n_samples=513, \n",
    "        rows=6, \n",
    "        cols=52839\n",
    "    )\n",
    "    \n",
    "    # storing similar m data\n",
    "    large_tau_data_samples.append(current_samples)\n",
    "    [large_tau_data_target.append(target) for target in current_targets]\n",
    "\n",
    "large_tau_data_samples = np.vstack(large_tau_data_samples)\n",
    "large_tau_data_target = np.array(large_tau_data_target)\n",
    "\n",
    "print(f\"Large Tau data shape: {large_tau_data_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2765f",
   "metadata": {},
   "source": [
    "For smaller tau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c915aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller Tau data shape: (30780, 1, 103, 6)\n"
     ]
    }
   ],
   "source": [
    "smaller_tau_data_samples = []\n",
    "smaller_tau_data_target = []\n",
    "\n",
    "for idx in np.arange(len(original_data)):\n",
    "    # splitting samples for smaller tau data\n",
    "    current_samples, current_targets = array_to_img(\n",
    "        smaller_tau_data[idx],\n",
    "        smaller_tau_target[idx],\n",
    "        n_samples=513, \n",
    "        rows=6, \n",
    "        cols=52839\n",
    "    )\n",
    "    \n",
    "    # storing similar m data\n",
    "    smaller_tau_data_samples.append(current_samples)\n",
    "    [smaller_tau_data_target.append(target) for target in current_targets]\n",
    "\n",
    "smaller_tau_data_samples = np.vstack(smaller_tau_data_samples)\n",
    "smaller_tau_data_target = np.array(smaller_tau_data_target)\n",
    "\n",
    "print(f\"Smaller Tau data shape: {smaller_tau_data_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b3d3c",
   "metadata": {},
   "source": [
    "And finally for similar Tau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b0b6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Tau data shape: (30780, 1, 103, 6)\n"
     ]
    }
   ],
   "source": [
    "similar_tau_data_samples = []\n",
    "similar_tau_data_target = []\n",
    "\n",
    "for idx in np.arange(len(original_data)):\n",
    "    # splitting samples for similar tau data\n",
    "    current_samples, current_targets = array_to_img(\n",
    "        similar_tau_data[idx],\n",
    "        similar_tau_target[idx],\n",
    "        n_samples=513, \n",
    "        rows=6, \n",
    "        cols=52839\n",
    "    )\n",
    "    \n",
    "    # storing similar m data\n",
    "    similar_tau_data_samples.append(current_samples)\n",
    "    [similar_tau_data_target.append(target) for target in current_targets]\n",
    "\n",
    "similar_tau_data_samples = np.vstack(similar_tau_data_samples)\n",
    "similar_tau_data_target = np.array(similar_tau_data_target)\n",
    "\n",
    "print(f\"Similar Tau data shape: {similar_tau_data_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de8161",
   "metadata": {},
   "source": [
    "## Data Serialization\n",
    "In this final step I'm serializing the pre-processed data to NPY files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data serialization\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"original_processed_data.npy\"),\n",
    "    original_data_samples,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"original_processed_targets.npy\"),\n",
    "    original_data_target,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "# similar m data serialization\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"similar_m_processed_data.npy\"),\n",
    "    similar_m_data_samples,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"similar_m_processed_targets.npy\"),\n",
    "    similar_m_data_target,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "# large tau data serialization\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"large_tau_processed_data.npy\"),\n",
    "    large_tau_data_samples,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"large_tau_processed_targets.npy\"),\n",
    "    large_tau_data_target,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "# smaller tau data serialization\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"smaller_tau_processed_data.npy\"),\n",
    "    smaller_tau_data_samples,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"smaller_tau_processed_targets.npy\"),\n",
    "    smaller_tau_data_target,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "# similar tau data serialization\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"similar_tau_processed_data.npy\"),\n",
    "    similar_tau_data_samples,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")\n",
    "\n",
    "np.save(\n",
    "    os.path.join(\"..\", \"data\", \"processed\", \"similar_tau_processed_targets.npy\"),\n",
    "    similar_tau_data_target,\n",
    "    allow_pickle=True,\n",
    "    fix_imports=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1074a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
